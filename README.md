# CreateED
Our repository for the hackathon CreateEd. 

We built CrossCom, which recognises hand gestures for 26 alphabets, 10 letters and a few phrases like I love you/how are you etc using a convolutional neural network based on tensorflow, keras, and numpy. It then converts those hand gestures to text and text to speech using pyttsx.

Also, the user can speak to crosscom as there as is a microphone in the camera (we couldn’t get a USB microphone unfortunately to plug into our raspberry pi) and that gets converted to text using the google cloud speech API, and comes out through the speaker behind. 

It is a communication device between somebody who’s deaf and mute, and somebody who’s blind.

CrossCom won the Accenture Challenge for best use of Machine Learning at the hackathon!

![alt text] https://github.com/akshayCha/CreateED/blob/master/CrossCom(1).jpg

![alt text] https://github.com/akshayCha/CreateED/blob/master/CrossCom(2).jpg

